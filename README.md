 # Validating ADAM and AmsGrad optimizers
 
With the popularity of Machine and Deep learning, the number of optimization algorithms keep increasing. However, we should question their reliability and limits before using them. Our aim in this paper is to present the comparison of the performance of some of these, in the context of a Deep Learning problem. For this, we used the MNIST dataset on popular architectures. More specifically, we compared the initial ADAM optimizer with the AMSGRAD.

### Repository description

The report of this work is the pdf: 'SGD-ADAM-AMSGRAD.pdf'
The code can be run through 'run.py'. This is also where it is possible to add a block and test a different setup (optimizer / meta-parameters). 
As running the whole code may be long, the generated data has been saved in the 'data' folder and the images used for the report have been saved in 'arrays and images'.
